\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{enumerate}
%\usepackage{float}
%\usepackage{calc}
\usepackage{indentfirst} 
\usepackage{multirow} 
%\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}} #2\end{tabular}}
%\usepackage{stfloats}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\ifcvprfinal\pagestyle{empty}\fi
% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}
	\title{Polyhedron Cone Classifier}
	\author{Lina Zhu\\\\June 24  ,2018}
%%%%%%%%% TITLE
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\maketitle
\section{Introduction}
 This article describes how linear discriminants overcome the current large-scale edge method in the sliding window for visual object detection and open recognition tasks. In these tasks, the classification problems are all digitized imbalanced positive training and testing windows are more rare than negative and geometric asymmetry. 4It introduces a series of quasilinear polyhedron cone discriminants. The methods have comparable properties and run-time complexity to Linear Support Vector Machines (SVMs), which can be used to train SVM constrained quadratic programming using binary or forward sampling. Our experiments show that they are significantly superior to both linear support vector machines and a wide range of existing discriminative object detection ranges, open set recognition and conventional closed classification tasks.

\par In our traditional machine learning classifier, each class seen at test time is known during training. These methods attempt to attribute each one even if there is almost no similarity between the test sample and the test sample, and it can be used as any known training sample of the test sample. To do this, they need to estimate a certain internal or verification area for each target category in addition to the traditional inter-class decision boundaries. For example, support vector machines treat two classes as equal, interchangeable alternatives. Both of these applications require reliable, scalable, and asymmetric discriminators that focus on the positive modeling class as a compact, consistent collection encompassing a different negative ocean. The trap for not doing so is shown in Figure~\ref{pic1}. This is an identification problem which unforeseen class happens at runtime, but it is the object detector that faces similar difficulties and unforeseen difficulties.
\begin{figure}[htp]
	\centering
	\includegraphics[width=6cm]{figure1.jpg}
	\caption{ The decision hyperplane returned by the SVM successfully separates the training class from dog (positive) and person (negative). However, it also assigns examples of new classes, such as cat classes for cats, horses, fish and chairs, and sometimes even higher confidence scores than dogs themselves.
	}\label{pic1}
\end{figure}

\section{Polyhedral Conic Classifiers}
Our classifier uses the polyhedral conic function of~\cite{Gasimov_2006_Separation}, basically determining the positive of their acceptance area by projecting the hyperplane partial cones. This option provides a convenient compact and convex series of area shapes with relatively well-distributed localized positive classes from the wider negative ones. It naturally allows for reliable edge-based learning, and the number of free parameters remains moderate, thus controlling over-decoration and runtime.Figure~\ref{pic2} illustrates that the pyramidal classifier 2D data set presented on the composition consists of random points consisting of Gaussian mean values. Table~\ref{table1} gives the empirical average accuracy obtained by statistically optimal Bayes to obtain the best accuracy classifier.
\begin{figure}[htp]
	\centering
	\includegraphics[width=6cm]{figure2.jpg}
	\caption{ For a multi-faceted pyramidal classifier, the frontal acceptance area is a kite-like axis-aligned octahedron containing points in linear form above (inside) the cone.
	}\label{pic2}
\end{figure}
\par 
The experimenter tests the proposed multi-face pyramid classifier. Both synthetic and real data sets for object detection are open set recognition and classic closed-set multi-class discrimination. The emphasis on the polyhedron classifier is best seen as a direct replacement for linear SVMs~\cite{Vedaldi_2012_Efficient}\cite{Mangasarian_2006_Multisurface}, systematically demonstrating excellent application and use of features in the following tests~\cite{Dundar_2008_Polyhedral}, with only modest increases in memory usage and runtime. Kernel support vector machines and similar instance-based methods are usually better and definitely more accurate, but they are usually too slow for practical use in these types of applications, except perhaps because the final stage of the classifier cascades faster, such as our method and other stages.


\begin{table}[tp]%[!hbp]
	\centering 
	\caption{Average Precision (\%) on the 2D synthetic dataset}\label{table1}
	%\resizebox{\textwidth}{25mm}{
	\tabcolsep 0.0001in 
	\begin{tabular} {|c|c|c|c|c|}
		\hline
		
		Method& AP Score (\%)\\
			\hline	\hline
		Bayes Optimal &90.89\\
			\hline
		EPCC &86.62\\
			\hline
		OC-EPCC& 84.87\\
			\hline
		PCC &79.90\\
			\hline
		Additive Kernels& 76.80\\
			\hline
		SVDD &71.14\\
			\hline
		GEPSVM& 44.25\\
			\hline
		SVM &22.85\\
	
		\hline
	\end{tabular}
\end{table}
\section{Conclusion}
Our findings yielded a solid piece of evidence showing that close to optimally sparse structural building blocks through easy-to-obtain denseness is a viable approach to improving neural networks for computer vision. The main advantage of this approach is that the computational requirements for significantly improved quality in the case of moderate increases are shallower and narrower architectures.


{\small
	\bibliographystyle{plain}
	\bibliography{ref}} 
\end{document}